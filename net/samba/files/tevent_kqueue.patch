diff --git a/lib/async_req/async_sock.c b/lib/async_req/async_sock.c
index 0a8a333f4f3..6071ff84420 100644
--- a/lib/async_req/async_sock.c
+++ b/lib/async_req/async_sock.c
@@ -273,7 +273,7 @@ struct tevent_req *writev_send(TALLOC_CTX *mem_ctx, struct tevent_context *ev,
 	if (tevent_req_nomem(state->iov, req)) {
 		return tevent_req_post(req, ev);
 	}
-	state->flags = TEVENT_FD_WRITE|TEVENT_FD_READ;
+	state->flags = TEVENT_FD_WRITE|TEVENT_FD_READ|TEVENT_FD_MULTIHANDLER;
 	state->err_on_readability = err_on_readability;
 
 	tevent_req_set_cleanup_fn(req, writev_cleanup);
@@ -472,7 +472,7 @@ struct tevent_req *read_packet_send(TALLOC_CTX *mem_ctx,
 	}
 
 	state->fde = tevent_add_fd(ev, state, fd,
-				   TEVENT_FD_READ, read_packet_handler,
+				   TEVENT_FD_READ|TEVENT_FD_MULTIHANDLER, read_packet_handler,
 				   req);
 	if (tevent_req_nomem(state->fde, req)) {
 		return tevent_req_post(req, ev);
diff --git a/lib/tevent/tevent.c b/lib/tevent/tevent.c
index dbec1821e41..0bf2b5cf3e4 100644
--- a/lib/tevent/tevent.c
+++ b/lib/tevent/tevent.c
@@ -132,6 +132,9 @@ static void tevent_backend_init(void)
 	tevent_epoll_init();
 #elif defined(HAVE_SOLARIS_PORTS)
 	tevent_port_init();
+#elif defined(HAVE_KQUEUE)
+	tevent_kqueue_init();
+
 #endif
 
 	tevent_standard_init();
diff --git a/lib/tevent/tevent.h b/lib/tevent/tevent.h
index 3c3e3cc2cef..1e132854351 100644
--- a/lib/tevent/tevent.h
+++ b/lib/tevent/tevent.h
@@ -32,6 +32,7 @@
 #include <talloc.h>
 #include <sys/time.h>
 #include <stdbool.h>
+#include <aio.h>
 
 struct tevent_context;
 struct tevent_ops;
@@ -491,6 +492,26 @@ void tevent_set_abort_fn(void (*abort_fn)(const char *reason));
  */
 #define TEVENT_FD_WRITE 2
 
+/**
+ * Code setting up handlers intentionally sets multiple
+ * handlers for reads or writes.
+ */
+#define TEVENT_FD_MULTIHANDLER 4
+
+#define TEVENT_FD_AIO_READ 8
+#define TEVENT_FD_AIO_WRITE 16
+#define TEVENT_FD_AIO_FSYNC 32
+#define TEVENT_FD_AIO (TEVENT_FD_AIO_READ|TEVENT_FD_AIO_WRITE|TEVENT_FD_AIO_FSYNC)
+
+struct tevent_aio_request {
+	struct tevent_req *req;
+	struct aiocb *iocbp;
+	bool in_progress;
+	size_t ret;
+	int saved_rv;
+	int saved_errno;
+};
+
 /**
  * Convenience function for declaring a tevent_fd writable
  */
diff --git a/lib/tevent/tevent_internal.h b/lib/tevent/tevent_internal.h
index 5365fce3533..db11c857bae 100644
--- a/lib/tevent/tevent_internal.h
+++ b/lib/tevent/tevent_internal.h
@@ -368,6 +368,8 @@ int tevent_common_loop_wait(struct tevent_context *ev,
 			    const char *location);
 
 int tevent_common_fd_destructor(struct tevent_fd *fde);
+int tevent_common_timed_destructor(struct tevent_timer *te);
+
 struct tevent_fd *tevent_common_add_fd(struct tevent_context *ev,
 				       TALLOC_CTX *mem_ctx,
 				       int fd,
@@ -466,6 +468,12 @@ void tevent_epoll_set_panic_fallback(struct tevent_context *ev,
 bool tevent_port_init(void);
 #endif
 
+#ifdef HAVE_KQUEUE
+bool tevent_kqueue_init(void);
+void tevent_kqueue_set_panic_fallback(struct tevent_context *ev,
+			bool (*panic_fallback)(struct tevent_context *ev,
+					       bool replay));
+#endif
 
 void tevent_trace_point_callback(struct tevent_context *ev,
 				 enum tevent_trace_point);
diff --git a/lib/tevent/tevent_kqueue.c b/lib/tevent/tevent_kqueue.c
new file mode 100755
index 00000000000..490d3b7d653
--- /dev/null
+++ b/lib/tevent/tevent_kqueue.c
@@ -0,0 +1,1172 @@
+/*
+   Unix SMB/CIFS implementation.
+
+   main select loop and event handling - kqueue implementation
+
+   Copyright (C) Andrew Tridgell	2003-2005
+   Copyright (C) Stefan Metzmacher	2005-2013
+   Copyright (C) Jeremy Allison		2013
+   Copyright (C) iXsystems		2020
+
+     ** NOTE! The following LGPL license applies to the tevent
+     ** library. This does NOT imply that all of Samba is released
+     ** under the LGPL
+
+   This library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU Lesser General Public
+   License as published by the Free Software Foundation; either
+   version 3 of the License, or (at your option) any later version.
+
+   This library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   Lesser General Public License for more details.
+
+   You should have received a copy of the GNU Lesser General Public
+   License along with this library; if not, see <http://www.gnu.org/licenses/>.
+*/
+#define TEVENT_DEPRECATED 1
+#include <sys/cdefs.h>
+#include "replace.h"
+#include <sys/event.h>
+#include <search.h>
+#include "system/filesys.h"
+#include "system/select.h"
+#include "tevent.h"
+#include "tevent_internal.h"
+#include "tevent_util.h"
+
+/* struct tevent_fd additional flags */
+#define KQUEUE_EXTRA_KEVENT_DISCHARGED	0x0001
+
+/* maximum number of kevents to process in loop_once*/
+#define MAX_KEVENTS			16
+
+#define TEVENT_FD_RW (TEVENT_FD_READ|TEVENT_FD_WRITE)
+#define HAS_AIO(flags) (flags & TEVENT_FD_AIO)
+#define HAS_WRITE(flags) (flags & TEVENT_FD_WRITE)
+#define HAS_READ(flags) (flags & TEVENT_FD_READ)
+#define HAS_READ_OR_WRITE(flags) (flags & TEVENT_FD_RW)
+#define HAS_READ_AND_WRITE(flags) ((flags & TEVENT_FD_RW) == TEVENT_FD_RW)
+#define KEVENT_TO_TEVENT_FLAGS(f) (abs(f) & TEVENT_FD_RW)
+#define TIMEVAL_IS_ZERO(tv) (tv.tv_sec == 0 && tv.tv_usec == 0)
+
+struct kqueue_signal_list {
+	struct kqueue_signal_list *prev, *next;
+	struct tevent_signal *se;
+	struct kqueue_event_context *kqueue_ev;
+	int signum;
+};
+
+struct kqueue_signal_handlers {
+	struct kqueue_signal_list *entries;
+	size_t nentries;
+};
+
+struct kqueue_event_context {
+	/* a pointer back to the generic event_context */
+	struct tevent_context *ev;
+
+	/* when using kqueue this is kernel event queue */
+	int kqueue_fd;
+	int kqueue_timer;
+	struct kqueue_signal_handlers sig_handlers[TEVENT_NUM_SIGNALS+1];
+
+	pid_t pid;
+
+	bool had_signal;
+	bool panic_force_replay;
+	bool *panic_state;
+	bool (*panic_fallback)(struct tevent_context *ev, bool replay);
+};
+
+static void kqueue_panic(struct kqueue_event_context *kqueue_ev,
+			 const char *reason, bool replay);
+
+static void kqueue_add_event(struct kqueue_event_context *kqueue_ev,
+			     struct tevent_fd *fde);
+
+static int kqueue_add_signal_int(struct kqueue_event_context *kqueue_ev,
+				 int signum, const char *handler_name);
+
+/*
+  called to set the panic fallback function.
+*/
+_PRIVATE_ void tevent_kqueue_set_panic_fallback(struct tevent_context *ev,
+				bool (*panic_fallback)(struct tevent_context *ev,
+						       bool replay))
+{
+	struct kqueue_event_context *kqueue_ev =
+		talloc_get_type_abort(ev->additional_data,
+		struct kqueue_event_context);
+
+	kqueue_ev->panic_fallback = panic_fallback;
+}
+
+/*
+  called when a kqueue call fails
+*/
+static void kqueue_panic(struct kqueue_event_context *kqueue_ev,
+			 const char *reason, bool replay)
+{
+	struct tevent_context *ev = kqueue_ev->ev;
+	bool (*panic_fallback)(struct tevent_context *ev, bool replay);
+
+	panic_fallback = kqueue_ev->panic_fallback;
+
+	if (kqueue_ev->panic_state != NULL) {
+		*kqueue_ev->panic_state = true;
+	}
+
+	if (kqueue_ev->panic_force_replay) {
+		replay = true;
+	}
+
+	if (panic_fallback == NULL) {
+		tevent_debug(ev, TEVENT_DEBUG_FATAL,
+			"%s (%s) replay[%u] - calling abort()\n",
+			reason, strerror(errno), (unsigned)replay);
+		abort();
+	}
+	if (!panic_fallback(ev, replay)) {
+		/* Fallback failed. */
+		tevent_debug(ev, TEVENT_DEBUG_FATAL,
+			"%s (%s) replay[%u] - calling abort()\n",
+			reason, strerror(errno), (unsigned)replay);
+		abort();
+	}
+}
+
+static inline struct tevent_fd *get_tevent_fd(struct kqueue_event_context *kqueue_ev,
+					      void *private, bool replay)
+{
+	if (private == NULL) {
+		kqueue_panic(kqueue_ev, "kevent failed", replay);
+	}
+	return (struct tevent_fd *)private;
+}
+
+
+static int kqueue_ctx_destructor(struct kqueue_event_context *kqueue_ev)
+{
+	close(kqueue_ev->kqueue_fd);
+	kqueue_ev->kqueue_fd = -1;
+	return 0;
+}
+
+static int do_kqueue(struct kqueue_event_context *kqueue_ev)
+{
+	kqueue_ev->kqueue_fd = kqueue();
+	if (kqueue_ev->kqueue_fd == -1) {
+		tevent_debug(kqueue_ev->ev, TEVENT_DEBUG_FATAL,
+			     "do_kqueue: Failed to create kqueue: %s \n",
+			     strerror(errno));
+		return -1;
+	}
+	if (!ev_set_close_on_exec(kqueue_ev->kqueue_fd)) {
+		tevent_debug(kqueue_ev->ev, TEVENT_DEBUG_WARNING,
+			     "do_kqueue: "
+			     "Failed to set close-on-exec on queue, "
+			     "file descriptor may be leaked to children.\n");
+	}
+	return 0;
+}
+
+static int kqueue_init_ctx(struct kqueue_event_context *kqueue_ev)
+{
+	int ret;
+
+	ret = do_kqueue(kqueue_ev);
+	if (ret != 0) {
+		return ret;
+	}
+	kqueue_ev->pid = getpid();
+	talloc_set_destructor(kqueue_ev, kqueue_ctx_destructor);
+
+	return 0;
+}
+
+/*
+  The kqueue queue is not inherited by a child created with fork(2).
+  So we need to re-initialize if this happens.
+ */
+static void kqueue_check_reopen(struct kqueue_event_context *kqueue_ev)
+{
+	struct tevent_fd *fde = NULL;
+	int ret, signum;
+	bool *caller_panic_state = kqueue_ev->panic_state;
+	bool panic_triggered = false;
+
+	if (kqueue_ev->pid == getpid()) {
+		return;
+	}
+	/*
+	 * We've forked. Re-initialize.
+	 */
+
+	close(kqueue_ev->kqueue_fd);
+	ret = do_kqueue(kqueue_ev);
+	if (ret != 0) {
+		kqueue_panic(kqueue_ev, "kqueue() failed", false);
+		return;
+	}
+	kqueue_ev->pid = getpid();
+	kqueue_ev->panic_state = &panic_triggered;
+	for (fde=kqueue_ev->ev->fd_events;fde;fde=fde->next) {
+		kqueue_add_event(kqueue_ev, fde);
+		if (panic_triggered) {
+			if (caller_panic_state != NULL) {
+				*caller_panic_state = true;
+			}
+			return;
+		}
+	}
+
+	/*
+	 * Re-init the kevents for signal handlers. Failure here
+	 * shouldn't be considered fatal because it's just a wrapper
+	 * wround the common event handler code, but of course, I'd
+	 * prefer it to work.
+	 */
+	for (signum = ARRAY_SIZE(kqueue_ev->sig_handlers) -1; signum >= 0; signum--){
+		if (kqueue_ev->sig_handlers[signum].nentries > 0) {
+			ret = kqueue_add_signal_int(kqueue_ev, signum, NULL);
+			if (ret != 0) {
+				tevent_debug(kqueue_ev->ev, TEVENT_DEBUG_FATAL,
+				     "do_kqueue: Failed to reinit signal [%d]: %s \n",
+				     signum, strerror(errno));
+			}
+		}
+	}
+
+	kqueue_ev->panic_state = NULL;
+}
+
+/*
+ * add the kqueue event to the given fd event.
+ * filter is removed in the destructor for the fd event.
+ */
+static void kqueue_add_event(struct kqueue_event_context *kqueue_ev, struct tevent_fd *fde)
+{
+	struct kevent event[2];
+	int ret, count;
+	struct tevent_fd *mpx_fde = NULL;
+	short kevent_filter = 0;
+	count = 0;
+	/*
+	 * kevents are uniquely identified by the tuple (queue, ident, and filter).
+	 * This means that if a tevent fd event has both TEVENT_FD_READ and TEVENT_FD_WRITE
+	 * flags set, then two separate kevents will need to be generated. Consequently,
+	 * an array of kevents must be retrieved in loop_once() so that the correct
+	 * tevent flags can be determined when calling the relevant handler function.
+	 * KQUEUE_HAS_EXTRA_KEVENT is used to indicate that this is the case.
+	 */
+	if (!HAS_READ_OR_WRITE(fde->flags)) {
+		return;
+	}
+
+	/*
+	 * In tevent, multiple fd events with overlapping flags may be registered for the fd.
+	 * On collision with existing kevent (same kqueue, fd, and filter), the existing
+	 * kevent will be overwritten with new data. Future room for improvement here is
+	 * to check if the existing kevent(s) already cover all the requisite filters. In
+	 * this case, then we can simply place a pointer to new tevent_fd as additional_data
+	 * in the existing tevent_fd.
+	 */
+	if (fde->flags & TEVENT_FD_MULTIHANDLER) {
+		for (mpx_fde = kqueue_ev->ev->fd_events; mpx_fde; mpx_fde = mpx_fde->next) {
+			if ((mpx_fde == fde) || (mpx_fde->fd != fde->fd)) {
+				continue;
+			}
+			fde->additional_data = mpx_fde;
+			break;
+		}
+	}
+
+	if (HAS_READ(fde->flags)) {
+		EV_SET(&event[0],		//kev
+		       fde->fd,			//ident
+		       EVFILT_READ,		//filter
+		       EV_ADD,			//flags
+		       0,			//fflags
+		       0,			//data
+		       fde);			//udata
+		count++;
+	}
+	if (HAS_WRITE(fde->flags)) {
+		EV_SET(&event[count],		//kev
+		       fde->fd,			//ident
+		       EVFILT_WRITE,		//filter
+		       EV_ADD,			//flags
+		       0,			//fflags
+		       0,			//data
+		       fde);			//udata
+		count++;
+	}
+	if (count) {
+		ret = kevent(kqueue_ev->kqueue_fd, event, count, NULL, 0, NULL);
+		if (ret != 0) {
+			goto failure;
+		}
+	}
+
+	return;
+
+failure:
+	if (errno == EBADF) {
+		tevent_debug(kqueue_ev->ev, TEVENT_DEBUG_FATAL,
+			     "KEVENT EBADF for "
+			     "fde[%p] fd[%d] - disabling\n",
+			     fde, fde->fd);
+		DLIST_REMOVE(kqueue_ev->ev->fd_events, fde);
+		fde->wrapper = NULL;
+		fde->event_ctx = NULL;
+		return;
+	} else {
+		kqueue_panic(kqueue_ev, "KEVENT add failed", false);
+		return;
+	}
+}
+
+static void kqueue_delete_event(struct kqueue_event_context *kqueue_ev,
+			        struct tevent_fd *fde,
+				uint16_t to_delete)
+{
+	struct kevent event[2];
+	int ret, count;
+	struct tevent_fd empty, *mpx_fde;
+	ZERO_STRUCT(empty);
+	mpx_fde = &empty;
+	count = 0;
+
+	/*
+	 * One or more flags is being removed from the fde. If the existing
+	 * kevent is associated with multiple tevent fd events, then the existing
+	 * kevent will need to be updated so that it is associated with one of the
+	 * un-discharged fd events.
+	 */
+
+	if (fde->additional_data != NULL) {
+		mpx_fde = (struct tevent_fd *)fde->additional_data;
+	}
+
+	if (HAS_READ(to_delete)) {
+		if (HAS_READ(mpx_fde->flags)) {
+			mpx_fde->additional_data = NULL;
+			EV_SET(&event[count],		//kev
+			       fde->fd,			//ident
+			       EVFILT_READ,		//filter
+			       EV_ADD,			//flags
+			       0,			//fflags
+			       0,			//data
+			       mpx_fde);		//udata
+		}
+		else {
+			EV_SET(&event[count],		//kev
+			       fde->fd,			//ident
+			       EVFILT_READ,		//filter
+			       EV_DELETE,		//flags
+			       0,			//fflags
+			       0,			//data
+			       0);			//udata
+		}
+		count++;
+	}
+	if (HAS_WRITE(to_delete)) {
+		if (HAS_WRITE(mpx_fde->flags)) {
+			mpx_fde->additional_data = NULL;
+			EV_SET(&event[count],		//kev
+			       fde->fd,			//ident
+			       EVFILT_WRITE,		//filter
+			       EV_ADD,			//flags
+			       0,			//fflags
+			       0,			//data
+			       mpx_fde);		//udata
+		}
+		else {
+			EV_SET(&event[count],		//kev
+			       fde->fd,			//ident
+			       EVFILT_WRITE,		//filter
+			       EV_DELETE,		//flags
+			       0,			//fflags
+			       0,			//data
+			       0);			//udata
+		}
+		count++;
+	}
+	if (count) {
+		ret = kevent(kqueue_ev->kqueue_fd, event, count, NULL, 0, NULL);
+		if (ret != 0 && errno != ENOENT) {
+			tevent_debug(kqueue_ev->ev, TEVENT_DEBUG_FATAL,
+				     "KEVENT_UPDATE_DEL failed "
+				     "for fde[%p] err: [%s]\n",
+				     fde, strerror(errno));
+		}
+	}
+}
+
+static int kqueue_timer_cleanup(struct tevent_timer *te)
+{
+	if (te->destroyed) {
+		tevent_common_check_double_free(te, "tevent_timer double free");
+	}
+	else {
+		te->destroyed = true;
+
+		if (te->event_ctx == NULL) {
+			return 0;
+		}
+		te->event_ctx = NULL;
+	}
+
+	if (te->busy) {
+		return -1;
+	}
+	te->wrapper = NULL;
+
+	return 0;
+}
+
+
+static int kqueue_event_timer_destructor(struct tevent_timer *te)
+{
+	int ret;
+	struct kevent event;
+	struct tevent_context *ev = NULL;
+	struct kqueue_event_context *kqueue_ev = NULL;
+
+	ev = te->event_ctx;
+	if (ev == NULL) {
+		return kqueue_timer_cleanup(te);
+	}
+	kqueue_ev = talloc_get_type_abort(ev->additional_data,
+					  struct kqueue_event_context);
+	if (!te->busy) {
+		EV_SET(&event, (uintptr_t)te, EVFILT_TIMER, EV_DELETE, 0, 0, 0);
+		ret = kevent(kqueue_ev->kqueue_fd, &event, 1, NULL, 0, NULL);
+		if (ret != 0 && errno != ENOENT) {
+			tevent_debug(kqueue_ev->ev, TEVENT_DEBUG_FATAL,
+				     "KEVENT_TIMER_DESTRUCTOR failed "
+				     "%s\n", strerror(errno));
+		}
+	}
+	return kqueue_timer_cleanup(te);
+}
+
+static int kqueue_event_fd_destructor(struct tevent_fd *fde)
+{
+	struct tevent_context *ev = fde->event_ctx;
+	struct kqueue_event_context *kqueue_ev = NULL;
+	bool panic_triggered = false;
+	int flags = fde->flags;
+
+	if (ev == NULL) {
+		return tevent_common_fd_destructor(fde);
+        }
+	kqueue_ev = talloc_get_type_abort(ev->additional_data,
+					  struct kqueue_event_context);
+	/*
+	 * we must remove the event from the list
+	 * otherwise a panic fallback handler may
+	 * reuse invalid memory
+	 */
+        DLIST_REMOVE(ev->fd_events, fde);
+
+	kqueue_ev->panic_state = &panic_triggered;
+	kqueue_check_reopen(kqueue_ev);
+	if (panic_triggered) {
+		return tevent_common_fd_destructor(fde);
+	}
+
+	kqueue_delete_event(kqueue_ev, fde, fde->flags);
+
+	if (panic_triggered) {
+		return tevent_common_fd_destructor(fde);
+	}
+	kqueue_ev->panic_state = NULL;
+
+	return tevent_common_fd_destructor(fde);
+}
+
+static int kqueue_signal_list_destructor(struct kqueue_signal_list *sl)
+{
+	int ret;
+	struct kqueue_signal_handlers *h = NULL;
+	struct kevent event;
+	if (sl->kqueue_ev == NULL) {
+		return 0;
+	}
+	h = &sl->kqueue_ev->sig_handlers[sl->signum];
+        if (h->nentries > 0) {
+                DLIST_REMOVE(h->entries, sl);
+		h->nentries--;
+        }
+	if (h->nentries == 0) {
+		EV_SET(&event,				//kev
+		       (uintptr_t)&sl->signum,		//ident
+		       EVFILT_SIGNAL,			//filter
+		       EV_DELETE,			//flags
+		       0,				//fflags
+		       0,				//data
+		       NULL);				//udata
+
+		ret = kevent(sl->kqueue_ev->kqueue_fd, &event, 1, NULL, 0, NULL);
+		if (ret != 0 && errno != ENOENT) {
+			tevent_debug(sl->kqueue_ev->ev, TEVENT_DEBUG_FATAL,
+				"failed to delete signal kevent for [%d]: %s\n",
+				sl->signum, strerror(errno));
+		}
+	}
+        return 0;
+}
+
+static struct timeval kqueue_get_current_time()
+{
+	struct timeval ret = tevent_timeval_zero();
+	struct timespec ts;
+	clock_gettime(CLOCK_REALTIME_FAST, &ts);
+	ret.tv_sec = ts.tv_sec;
+	ret.tv_usec = ts.tv_nsec / 1000;
+	return ret;
+}
+
+static int kqueue_process_timer(struct kqueue_event_context *kqueue_ev,
+					void *udata)
+{
+	int ret;
+	struct tevent_timer *te = NULL;
+	struct tevent_context *handler_ev = NULL;
+
+	struct timeval current_time = kqueue_get_current_time();
+
+	te = (struct tevent_timer *)udata;
+	if (te->event_ctx == NULL) {
+		return 0;
+	}
+
+	handler_ev = te->event_ctx;
+	te->busy = true;
+	if (te->wrapper != NULL) {
+		handler_ev = te->wrapper->wrap_ev;
+		tevent_wrapper_push_use_internal(handler_ev, te->wrapper);
+		te->wrapper->ops->before_timer_handler(
+					te->wrapper->wrap_ev,
+					te->wrapper->private_state,
+					te->wrapper->main_ev,
+					te,
+					te->next_event,
+					current_time,
+					te->handler_name,
+					te->location);
+	}
+	te->handler(handler_ev, te, current_time, te->private_data);
+	if (te->wrapper != NULL) {
+		te->wrapper->ops->after_timer_handler(
+					te->wrapper->wrap_ev,
+					te->wrapper->private_state,
+					te->wrapper->main_ev,
+					te,
+					te->next_event,
+					current_time,
+					te->handler_name,
+					te->location);
+		tevent_wrapper_pop_use_internal(handler_ev, te->wrapper);
+	}
+	te->wrapper = NULL;
+	te->event_ctx = NULL;
+	talloc_set_destructor(te, NULL);
+	te->busy = false;
+	return 0;
+}
+
+static int kqueue_process_fd(struct kqueue_event_context *kqueue_ev,
+			     struct tevent_fd *fde, uint16_t flags)
+{
+	int ret;
+	struct tevent_fd *mpx_fde = NULL;
+	/*
+	 * If the fd being monitored has multiple tevent fd events registered for
+	 * it, then invoke their fd handlers.
+	 */
+	if (fde->additional_data != NULL) {
+		mpx_fde = (struct tevent_fd *)fde->additional_data;
+
+		tevent_common_invoke_fd_handler(fde,
+					        fde->flags & flags,
+					        NULL);
+
+		return tevent_common_invoke_fd_handler(mpx_fde,
+						       mpx_fde->flags & flags,
+						       NULL);
+	}
+
+	return tevent_common_invoke_fd_handler(fde, fde->flags & flags, NULL);
+}
+
+static inline void kqueue_process_aio(void *udata)
+{
+	struct tevent_aio_request *aio_req = NULL;
+	aio_req = talloc_get_type_abort(udata, struct tevent_aio_request);
+	if (!aio_req->in_progress) {
+		return;
+	}
+	aio_req->in_progress = false;
+	aio_req->ret = aio_return(aio_req->iocbp);
+	aio_req->saved_errno = errno;
+	tevent_req_done(aio_req->req);
+}
+
+static inline void kqueue_process_signal(struct kqueue_event_context *kqueue_ev,
+				  uint signum, int count)
+{
+	if (kqueue_ev->had_signal) {
+		return;
+	}
+
+	if (kqueue_ev->ev->signal_events) {
+	    tevent_common_check_signal(kqueue_ev->ev);
+	}
+}
+
+static inline void kqueue_process_kev(struct kqueue_event_context *kqueue_ev,
+				      struct kevent kev, struct tevent_fd **fde,
+				      bool *is_fd_event)
+{
+	switch (kev.filter) {
+	case EVFILT_AIO:
+		kqueue_process_aio(kev.udata);
+		*is_fd_event = false;
+		break;
+	case EVFILT_TIMER:
+		kqueue_process_timer(kqueue_ev, kev.udata);
+		*is_fd_event = false;
+		break;
+	case EVFILT_WRITE:
+	case EVFILT_READ:
+		*fde = get_tevent_fd(kqueue_ev, kev.udata, true);
+		*is_fd_event = true;
+		break;
+	case EVFILT_SIGNAL:
+		kqueue_process_signal(kqueue_ev, (uint)kev.ident, (int)kev.data);
+		*is_fd_event = false;
+		break;
+	default:
+		kqueue_panic(kqueue_ev, "Unexpected kevent filter: %d",
+			     kev.filter);
+	}
+	return;
+}
+
+static inline int process_single_kevent(struct kqueue_event_context *kqueue_ev, int nkevents,
+				 struct kevent *received)
+{
+	struct tevent_fd *fde = NULL;
+	bool is_fd_event;
+	uint16_t flags = 0;
+	kqueue_process_kev(kqueue_ev, received[0], &fde, &is_fd_event);
+	if (fde != NULL) {
+		flags = KEVENT_TO_TEVENT_FLAGS(received[0].filter);
+		flags &= fde->flags;
+		return kqueue_process_fd(kqueue_ev, fde, flags);
+	}
+	return 0;
+}
+
+static inline int process_two_kevents(struct kqueue_event_context *kqueue_ev, int nkevents,
+			       struct kevent *received)
+{
+	int ret;
+	struct tevent_fd *fde = NULL;
+	struct tevent_fd *next_fde = NULL;
+	bool is_fd_event;
+	uint16_t flags = 0;
+
+	kqueue_process_kev(kqueue_ev, received[0], &fde, &is_fd_event);
+	kqueue_process_kev(kqueue_ev, received[1], &next_fde, &is_fd_event);
+
+	if (fde == NULL && next_fde == NULL) {
+		//No fd events and so don't bother with handler.
+		return 0;
+	}
+
+	/*
+	 * Both kevents are for the same tevent_fd event.
+	 * Combine flags and move along.
+	 */
+	else if (fde == next_fde) {
+		return kqueue_process_fd(kqueue_ev, fde, TEVENT_FD_RW);
+	}
+	/*
+	 * Only one (fd, kqueue filter) combination may be registered
+	 * at a time. It's theoretically possible that two separate
+	 * tevent fd events have registered kevents on the same fd.
+	 * These should have been made consistent in kqueue_add_event().
+	 */
+	else if ((fde && next_fde) && (fde->fd == next_fde->fd)) {
+		flags = TEVENT_FD_RW;
+		return kqueue_process_fd(kqueue_ev, fde, flags);
+	}
+
+	if (fde) {
+		flags = KEVENT_TO_TEVENT_FLAGS(received[0].filter);
+		flags &= fde->flags;
+		ret = kqueue_process_fd(kqueue_ev, fde, flags);
+	}
+	if (next_fde) {
+		flags = KEVENT_TO_TEVENT_FLAGS(received[1].filter);
+		flags &= next_fde->flags;
+		ret = kqueue_process_fd(kqueue_ev, next_fde, flags);
+	}
+	return ret;
+}
+
+static inline int process_many_kevents(struct kqueue_event_context *kqueue_ev, int nkevents,
+				struct kevent *received)
+{
+	int rv, i, iloop;
+	struct tevent_fd *fde = NULL;
+	struct tevent_fd *next_fde = NULL;
+	struct tevent_fd *found_fde = NULL;
+	bool is_fd_event;
+	uint16_t flags = 0;
+	rv = 0;
+
+	for (i=0; i<nkevents; i++) {
+		kqueue_process_kev(kqueue_ev, received[i], &fde, &is_fd_event);
+		if (!is_fd_event) {
+			continue;
+		}
+
+		if (fde->additional_flags & KQUEUE_EXTRA_KEVENT_DISCHARGED) {
+			/* we fired off handler, reset flag and skip */
+			fde->additional_flags &= ~KQUEUE_EXTRA_KEVENT_DISCHARGED;
+			continue;
+		}
+
+		flags = KEVENT_TO_TEVENT_FLAGS(received[i].filter);
+
+		if (HAS_READ_AND_WRITE(fde->flags)) {
+			if (fde->additional_data != NULL) {
+				next_fde = (struct tevent_fd *)fde->additional_data;
+			}
+			for (iloop=i+1;iloop<nkevents;iloop++) {
+				if (received[iloop].filter & ~(EVFILT_READ|EVFILT_WRITE)) {
+					continue;
+				}
+				found_fde = get_tevent_fd(kqueue_ev,
+							  received[iloop].udata,
+							  true);
+				if (found_fde == fde) {
+					found_fde->additional_flags |= KQUEUE_EXTRA_KEVENT_DISCHARGED;
+					flags |= KEVENT_TO_TEVENT_FLAGS(received[iloop].filter);
+				}
+				else if (next_fde && (next_fde->fd == found_fde->fd)) {
+					found_fde->additional_flags |= KQUEUE_EXTRA_KEVENT_DISCHARGED;
+					flags |= KEVENT_TO_TEVENT_FLAGS(received[iloop].filter);
+				}
+			}
+		}
+		rv = kqueue_process_fd(kqueue_ev, fde, flags);
+	}
+	return rv;
+}
+
+static int kqueue_event_loop(struct kqueue_event_context *kqueue_ev, int *nkp)
+{
+	/*
+	 * Loop through an array of kevents. We need to retrieve multiple events at once
+	 * in order to properly reconstruct the correct flags to send to the tevent
+	 * fd handler.
+	 */
+	int nkevents, ret;
+	struct kevent received[MAX_KEVENTS];
+	struct timespec ts = {.tv_sec = 1, .tv_nsec = 0};
+	int wait_errno = 0;
+	ret = 0;
+	nkevents = 0;
+	/*
+	 * If timeout is a non-NULL pointer, it specifies a maximum interval
+	 * to wait for an event. If timeout is a NULL pointer, kevent() waits
+	 * indefinetly. To effect a poll, the timeout argument should be non-NULL,
+	 * pointing to a zero-valued timespec structure.
+	 */
+
+	tevent_trace_point_callback(kqueue_ev->ev, TEVENT_TRACE_BEFORE_WAIT);
+	nkevents = kevent(kqueue_ev->kqueue_fd,
+			  NULL,			/* changelist */
+			  0,			/* nchanges */
+			  received,		/* eventlist */
+			  MAX_KEVENTS,		/* nevents */
+			  &ts);			/* timeout  - may need NULL*/
+
+	wait_errno = errno;
+	tevent_trace_point_callback(kqueue_ev->ev, TEVENT_TRACE_AFTER_WAIT);
+	*nkp = nkevents;
+
+	if (nkevents == -1) {
+		/*
+		 * One likely cause for kevent() to fail with EBADF is that
+		 * we forked. In this case call kqueue_check_reopen() to reinitialize
+		 * the kqueue.
+		 */
+		if (wait_errno == EBADF) {
+			kqueue_check_reopen(kqueue_ev);
+			if (kqueue_ev->panic_state) {
+				tevent_debug(kqueue_ev->ev, TEVENT_DEBUG_FATAL,
+					     "panic_tiggered in kqueue event loop once\n");
+				errno = EINVAL;
+				return -1;
+			}
+			nkevents = kevent(kqueue_ev->kqueue_fd, NULL, 0,
+					  received, MAX_KEVENTS, &ts);
+		}
+		/*
+		 * kevent() returns -1 with errno EINTR if signal
+		 * received before timeout expired and event events
+		 * placed on the kqueue for return.
+		 */
+		else if (wait_errno != EINTR) {
+			tevent_debug(kqueue_ev->ev, TEVENT_DEBUG_FATAL,
+				"kevent() failed: %s\n",strerror(wait_errno));
+			kqueue_panic(kqueue_ev, "kevent() failed", false);
+		}
+		return 0;
+	}
+	if (nkevents == 1) {
+		ret = process_single_kevent(kqueue_ev, nkevents, received);
+	}
+	else if (nkevents == 2) {
+		ret = process_two_kevents(kqueue_ev, nkevents, received);
+	}
+	else {
+		ret = process_many_kevents(kqueue_ev, nkevents, received);
+	}
+	kqueue_ev->had_signal = false;
+	return ret;
+}
+
+static int kqueue_event_context_init(struct tevent_context *ev)
+{
+	int ret;
+	struct kqueue_event_context *kqueue_ev;
+
+	/*
+	 * We might be called during tevent_re_initialise()
+	 * which means we need to free our old additional_data.
+	 */
+	kqueue_ev = talloc_zero(ev, struct kqueue_event_context);
+
+	if (!kqueue_ev) return -1;
+	kqueue_ev->ev = ev;
+	kqueue_ev->kqueue_fd = -1;
+
+	ret = kqueue_init_ctx(kqueue_ev);
+	if (ret != 0) {
+		talloc_free(kqueue_ev);
+		return ret;
+	}
+
+	ev->additional_data = kqueue_ev;
+	return 0;
+}
+
+static struct tevent_fd *kqueue_add_aio(struct kqueue_event_context *kqueue_ev,
+					uint16_t flags,
+					void *private_data)
+{
+	int ret;
+	struct tevent_aio_request *taio = NULL;
+	taio = talloc_get_type_abort(private_data,
+				     struct tevent_aio_request);
+
+	taio->iocbp->aio_sigevent.sigev_notify_kqueue = kqueue_ev->kqueue_fd;
+
+	switch (flags & TEVENT_FD_AIO) {
+	case TEVENT_FD_AIO_READ:
+		ret = aio_read(taio->iocbp);
+		break;
+	case TEVENT_FD_AIO_WRITE:
+		ret = aio_write(taio->iocbp);
+		break;
+	case TEVENT_FD_AIO_FSYNC:
+		ret = aio_fsync(O_SYNC, taio->iocbp);
+		break;
+	}
+	if (ret != 0) {
+		taio->ret = ret;
+		taio->saved_errno = errno;
+	}
+	// Caller gets return value and errno from aio_request
+	return NULL;
+}
+
+static struct tevent_fd *kqueue_event_add_fd(struct tevent_context *ev, TALLOC_CTX *mem_ctx,
+					     int fd, uint16_t flags,
+					     tevent_fd_handler_t handler,
+					     void *private_data,
+					     const char *handler_name,
+					     const char *location)
+{
+	struct kqueue_event_context *kqueue_ev =
+		talloc_get_type_abort(ev->additional_data,
+		struct kqueue_event_context);
+	struct tevent_fd *fde = NULL;
+	bool panic_triggered = false;
+
+	if (HAS_AIO(flags)) {
+		return kqueue_add_aio(kqueue_ev, flags, private_data);
+	}
+
+	fde = tevent_common_add_fd(ev, mem_ctx, fd, flags,
+				   handler, private_data,
+				   handler_name, location);
+	if (!fde) {
+		tevent_debug(ev, TEVENT_DEBUG_FATAL,
+			"tevent_common_add_fd() failed\n");
+		return NULL;
+	}
+	talloc_set_destructor(fde, kqueue_event_fd_destructor);
+	kqueue_ev->panic_state = &panic_triggered;
+	kqueue_check_reopen(kqueue_ev);
+	if (panic_triggered) {
+		return fde;
+	}
+	kqueue_ev->panic_state = NULL;
+
+	kqueue_add_event(kqueue_ev, fde);
+
+	return fde;
+}
+
+static void kqueue_event_set_fd_flags(struct tevent_fd *fde, uint16_t flags)
+{
+	struct tevent_context *ev;
+	struct kqueue_event_context *kqueue_ev;
+	bool panic_triggered = false;
+	uint16_t to_delete = 0;
+
+	if (fde->flags == flags) return;
+
+	to_delete = fde->flags & ~flags;
+
+	ev = fde->event_ctx;
+
+	kqueue_ev = talloc_get_type_abort(ev->additional_data,
+					  struct kqueue_event_context);
+	fde->flags = flags;
+	kqueue_ev->panic_state = &panic_triggered;
+	kqueue_check_reopen(kqueue_ev);
+	if (panic_triggered) {
+		return;
+	}
+	kqueue_ev->panic_state = NULL;
+
+	kqueue_delete_event(kqueue_ev, fde, to_delete);
+
+	kqueue_add_event(kqueue_ev, fde);
+}
+
+struct tevent_timer *kqueue_event_add_timer(struct tevent_context *ev,
+					    TALLOC_CTX *mem_ctx,
+					    struct timeval next_event,
+					    tevent_timer_handler_t handler,
+					    void *private_data,
+					    const char *handler_name,
+					    const char *location)
+{
+	int ret;
+	struct tevent_timer *te = NULL;
+	struct kqueue_event_context *kqueue_ev = NULL;
+	struct kevent event;
+	int64_t usec_time;
+	uint fflags = NOTE_USECONDS;
+
+	kqueue_ev = talloc_get_type_abort(ev->additional_data,
+					  struct kqueue_event_context);
+
+        te = talloc(mem_ctx?mem_ctx:ev, struct tevent_timer);
+        if (te == NULL) return NULL;
+
+        *te = (struct tevent_timer) {
+                .event_ctx      = ev,
+                .next_event     = next_event,
+                .handler        = handler,
+                .private_data   = private_data,
+                .handler_name   = handler_name,
+                .location       = location,
+        };
+
+	usec_time = ((next_event.tv_sec) * 1000000) + next_event.tv_usec;
+
+	/*
+	 * Legacy tevent usage may have passed zeroed tval for
+	 * immediate events. In this case, usec_time is zero and
+	 * so we only need to _not_ set NOTE_ABSTIME to effect an
+	 * immediate firing of timer.
+	 */
+	if (!TIMEVAL_IS_ZERO(next_event)) {
+		fflags |= NOTE_ABSTIME;
+	}
+
+	/*
+	 * Timer will be periodic unless EV_ONESHOT or NOTE_ABSTIME
+	 * are set. On return "data" contains the number of times
+	 * the timeout expired since the last call to kevent(). For
+	 * non-monotonic timers, this filter automatically sets EV_CLEAR.
+	 */
+	EV_SET(&event,				//kev
+	       (uintptr_t)te,			//ident
+	       EVFILT_TIMER,			//filter
+	       EV_ADD|EV_ONESHOT,		//flags
+	       fflags,				//fflags
+	       usec_time,			//data
+	       te);				//udata
+
+	talloc_set_destructor(te, kqueue_event_timer_destructor);
+	ret = kevent(kqueue_ev->kqueue_fd, &event, 1, NULL, 0, NULL);
+	if (ret != 0) {
+		tevent_debug(ev, TEVENT_DEBUG_FATAL,
+			"failed to add timed kevent for [%s]\n",
+			handler_name);
+	}
+	return te;
+}
+
+/*
+ * Add a wrapper around tevent_common_add_signal(). This is to make it
+ * so that we process signals more or less immediately when we gather and process
+ * kevents (timeout).
+ */
+static int kqueue_add_signal_int(struct kqueue_event_context *kqueue_ev,
+				 int signum, const char *handler_name)
+{
+	int ret;
+	struct kevent event;
+	EV_SET(&event,				//kev
+	       (uintptr_t)&signum,		//ident
+	       EVFILT_SIGNAL,			//filter
+	       EV_ADD,				//flags
+	       0,				//fflags
+	       0,				//data
+	       NULL);				//udata
+
+	ret = kevent(kqueue_ev->kqueue_fd, &event, 1, NULL, 0, NULL);
+	if (ret != 0) {
+		tevent_debug(kqueue_ev->ev, TEVENT_DEBUG_FATAL,
+			"failed to add signal kevent for [%d] with handler "
+			"[%s]: %s\n", signum, handler_name, strerror(errno));
+	}
+	return ret;
+}
+
+struct tevent_signal *kqueue_event_add_signal(struct tevent_context *ev,
+					      TALLOC_CTX *mem_ctx,
+					      int signum,
+					      int sa_flags,
+					      tevent_signal_handler_t handler,
+					      void *private_data,
+					      const char *handler_name,
+					      const char *location)
+{
+	struct tevent_signal *se = NULL;
+	struct kqueue_signal_list *sl = NULL;
+	struct kqueue_event_context *kqueue_ev = NULL;
+	struct kqueue_signal_handlers *handlers = NULL;
+	struct kevent event;
+	int ret;
+
+	if (signum >= TEVENT_NUM_SIGNALS) {
+		errno = EINVAL;
+		return NULL;
+	}
+
+	kqueue_ev = talloc_get_type_abort(ev->additional_data,
+					  struct kqueue_event_context);
+
+	se = tevent_common_add_signal(ev, mem_ctx, signum, sa_flags,
+				      handler, private_data,
+				      handler_name, location);
+
+	sl = talloc_zero(se, struct kqueue_signal_list);
+	if (!sl) {
+		talloc_free(se);
+		return NULL;
+	}
+
+	sl->se = se;
+	sl->signum = signum;
+	sl->kqueue_ev = kqueue_ev;
+	handlers = &kqueue_ev->sig_handlers[signum];
+	/* only install a signal handler if not already installed */
+	if (handlers->nentries == 0) {
+		kqueue_add_signal_int(kqueue_ev, signum, handler_name);
+	}
+	DLIST_ADD(handlers->entries, sl);
+	handlers->nentries++;
+	tevent_debug(ev, TEVENT_DEBUG_TRACE,
+		"added signal [%u] kevent for [%s] nentries: %zu\n",
+		signum, handler_name, handlers->nentries);
+
+        talloc_set_destructor(sl, kqueue_signal_list_destructor);
+        return se;
+}
+
+/*
+  do a single event loop using the events defined in ev
+*/
+static int kqueue_event_loop_once(struct tevent_context *ev, const char *location)
+{
+	struct kqueue_event_context *kqueue_ev =
+		talloc_get_type_abort(ev->additional_data,
+		struct kqueue_event_context);
+	int ret;
+	int nkevents = MAX_KEVENTS;
+	bool panic_triggered = false;
+
+	if (ev->signal_events &&
+	    tevent_common_check_signal(ev)) {
+		kqueue_ev->had_signal = true;
+		return 0;
+	}
+
+	if (ev->threaded_contexts != NULL) {
+		tevent_common_threaded_activate_immediate(ev);
+	}
+
+	if (ev->immediate_events &&
+	    tevent_common_loop_immediate(ev)) {
+		return 0;
+	}
+
+	kqueue_ev->panic_state = &panic_triggered;
+	kqueue_ev->panic_force_replay = true;
+	/*
+	 * Loops untile we are no longer overflowing on kevents
+	 * This can happen if large number of AIO kevents or
+	 * immediate events have been submitted since last loop.
+	 */
+	while (nkevents == MAX_KEVENTS) {
+		ret = kqueue_event_loop(kqueue_ev, &nkevents);
+	}
+	kqueue_ev->panic_force_replay = false;
+	kqueue_ev->panic_state = NULL;
+	return ret;
+}
+
+static const struct tevent_ops kqueue_event_ops = {
+	.context_init		= kqueue_event_context_init,
+	.add_fd			= kqueue_event_add_fd,
+	.set_fd_close_fn	= tevent_common_fd_set_close_fn,
+	.get_fd_flags		= tevent_common_fd_get_flags,
+	.set_fd_flags		= kqueue_event_set_fd_flags,
+	.add_timer		= kqueue_event_add_timer,
+	.schedule_immediate	= tevent_common_schedule_immediate,
+	.add_signal		= kqueue_event_add_signal,
+	.loop_once		= kqueue_event_loop_once,
+	.loop_wait		= tevent_common_loop_wait,
+};
+
+_PRIVATE_ bool tevent_kqueue_init(void)
+{
+	return tevent_register_backend("kqueue", &kqueue_event_ops);
+}
diff --git a/lib/tevent/tevent_standard.c b/lib/tevent/tevent_standard.c
index 38720204e02..6ad32edb9a7 100644
--- a/lib/tevent/tevent_standard.c
+++ b/lib/tevent/tevent_standard.c
@@ -54,9 +54,13 @@ static const struct tevent_ops std_event_ops = {
   Move us to using poll instead. If we return false here,
   caller should abort().
 */
-#ifdef HAVE_EPOLL
+#if defined(HAVE_EPOLL) || defined(HAVE_KQUEUE)
 static bool std_fallback_to_poll(struct tevent_context *ev, bool replay)
 {
+
+        tevent_debug(ev, TEVENT_DEBUG_ERROR,
+                     "fallback\n");
+
 	void *glue_ptr = talloc_parent(ev->ops);
 	struct std_event_glue *glue =
 		talloc_get_type_abort(glue_ptr,
@@ -171,8 +175,11 @@ static int std_event_context_init(struct tevent_context *ev)
 		if (glue == NULL) {
 			return -1;
 		}
-
+#ifdef HAVE_KQUEUE
+		glue->epoll_ops = tevent_find_ops_byname("kqueue");
+#else
 		glue->epoll_ops = tevent_find_ops_byname("epoll");
+#endif
 
 		glue->poll_ops = tevent_find_ops_byname("poll");
 		if (glue->poll_ops == NULL) {
@@ -216,6 +223,9 @@ static int std_event_context_init(struct tevent_context *ev)
 #ifdef HAVE_EPOLL
 		tevent_epoll_set_panic_fallback(ev, std_fallback_to_poll);
 #endif
+#ifdef HAVE_KQUEUE
+		tevent_kqueue_set_panic_fallback(ev, std_fallback_to_poll);
+#endif
 
 		return ret;
 	}
diff --git a/lib/tevent/tevent_timed.c b/lib/tevent/tevent_timed.c
index a78d286a187..a6fc65602fc 100644
--- a/lib/tevent/tevent_timed.c
+++ b/lib/tevent/tevent_timed.c
@@ -132,7 +132,7 @@ struct timeval tevent_timeval_current_ofs(uint32_t secs, uint32_t usecs)
 /*
   destroy a timed event
 */
-static int tevent_common_timed_destructor(struct tevent_timer *te)
+int tevent_common_timed_destructor(struct tevent_timer *te)
 {
 	if (te->destroyed) {
 		tevent_common_check_double_free(te, "tevent_timer double free");
diff --git a/lib/tevent/wscript b/lib/tevent/wscript
index 93af416f583..92882cad476 100644
--- a/lib/tevent/wscript
+++ b/lib/tevent/wscript
@@ -43,6 +43,9 @@ def configure(conf):
     if conf.CHECK_FUNCS('epoll_create', headers='sys/epoll.h'):
         conf.DEFINE('HAVE_EPOLL', 1)
 
+    if conf.CHECK_FUNCS('kqueue', headers='sys/event.h'):
+        conf.DEFINE('HAVE_KQUEUE', 1)
+
     tevent_num_signals = 64
     v = conf.CHECK_VALUEOF('NSIG', headers='signal.h')
     if v is not None:
@@ -79,6 +82,9 @@ def build(bld):
     if bld.CONFIG_SET('HAVE_EPOLL'):
         SRC += ' tevent_epoll.c'
 
+    if bld.CONFIG_SET('HAVE_KQUEUE'):
+        SRC += ' tevent_kqueue.c'
+
     if bld.CONFIG_SET('HAVE_SOLARIS_PORTS'):
         SRC += ' tevent_port.c'
 
